{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64dd011f",
   "metadata": {},
   "source": [
    "Part 1 : K-Fold Cross Validation for Multiple Linear Regression (Least Square Error Fit)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9fccd6",
   "metadata": {},
   "source": [
    "(a) Divide the dataset into input features (all columns except price) and output variable  \n",
    "(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77abde0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a) Input features (X) shape: (5000, 5)\n",
      "(a) Output variable (y) shape: (5000, 1)\n",
      "First 5 rows of X:\n",
      " [[7.95454586e+04 5.68286132e+00 7.00918814e+00 4.09000000e+00\n",
      "  2.30868005e+04]\n",
      " [7.92486424e+04 6.00289981e+00 6.73082102e+00 3.09000000e+00\n",
      "  4.01730722e+04]\n",
      " [6.12870672e+04 5.86588984e+00 8.51272743e+00 5.13000000e+00\n",
      "  3.68821594e+04]\n",
      " [6.33452401e+04 7.18823609e+00 5.58672866e+00 3.26000000e+00\n",
      "  3.43102428e+04]\n",
      " [5.99821972e+04 5.04055452e+00 7.83938779e+00 4.23000000e+00\n",
      "  2.63541095e+04]]\n",
      "First 5 values of y:\n",
      " [[1059033.558 ]\n",
      " [1505890.915 ]\n",
      " [1058987.988 ]\n",
      " [1260616.807 ]\n",
      " [ 630943.4893]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df = pd.read_csv(\"USA_Housing.csv\")\n",
    "\n",
    "X = df.drop(\"Price\", axis=1).values   # all columns except Price\n",
    "y = df[\"Price\"].values.reshape(-1, 1) # target variable\n",
    "\n",
    "print(\"(a) Input features (X) shape:\", X.shape)\n",
    "print(\"(a) Output variable (y) shape:\", y.shape)\n",
    "print(\"First 5 rows of X:\\n\", X[:5])\n",
    "print(\"First 5 values of y:\\n\", y[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56ef92",
   "metadata": {},
   "source": [
    "(b) Scale the values of input features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce901b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(b) Scaled features (first 5 rows):\n",
      " [[ 1.02865969 -0.29692705  0.02127433  0.08806222 -1.31759867]\n",
      " [ 1.00080775  0.02590164 -0.25550611 -0.72230146  0.40399945]\n",
      " [-0.68462915 -0.11230283  1.5162435   0.93084045  0.07240989]\n",
      " [-0.49149907  1.22157207 -1.39307717 -0.58453963 -0.18673422]\n",
      " [-0.80707253 -0.94483368  0.84674187  0.20151314 -0.98838741]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\n(b) Scaled features (first 5 rows):\\n\", X_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8c271",
   "metadata": {},
   "source": [
    "(c) Divide input and output features into five folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805f2f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(c) Prepared 5-fold cross validation.\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"\\n(c) Prepared 5-fold cross validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dcb320",
   "metadata": {},
   "source": [
    "(d) Run five iterations, in each iteration consider one-fold as test set and remaining \n",
    "four sets as training set. Find the beta (ùõΩ) matrix, predicted values, and R2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0181cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "Beta matrix:\n",
      " [1232002.6748241   230745.9407348   163243.27314515  120309.77397759\n",
      "    3011.45976111  151552.63069359]\n",
      "R2 Score: 0.9179971706985147\n",
      "\n",
      "--- Fold 2 ---\n",
      "Beta matrix:\n",
      " [1232037.85755945  229081.97914235  165882.1605634   121536.57475055\n",
      "    2092.4478622   150874.99274586]\n",
      "R2 Score: 0.9145677884802819\n",
      "\n",
      "--- Fold 3 ---\n",
      "Beta matrix:\n",
      " [1231951.92563846  230224.50511001  162766.17455493  121022.77324577\n",
      "    1247.16258975  150234.77720419]\n",
      "R2 Score: 0.9116116385364478\n",
      "\n",
      "--- Fold 4 ---\n",
      "Beta matrix:\n",
      " [1232751.4648651   229500.10043209  165212.07110924  122839.9376815\n",
      "    3063.71699324  150917.88484984]\n",
      "R2 Score: 0.9193091764960818\n",
      "\n",
      "--- Fold 5 ---\n",
      "Beta matrix:\n",
      " [1.23161736e+06 2.30225051e+05 1.63956839e+05 1.21115120e+05\n",
      " 7.83467170e+02 1.50662447e+05]\n",
      "R2 Score: 0.9243869413350317\n"
     ]
    }
   ],
   "source": [
    "r2_scores = []\n",
    "betas = []\n",
    "fold = 1\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_scaled):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    \n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Add bias column (intercept)\n",
    "    X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "    X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "    \n",
    "    # Compute Beta\n",
    "    beta = np.linalg.inv(X_train_b.T @ X_train_b) @ (X_train_b.T @ y_train)\n",
    "    print(\"Beta matrix:\\n\", beta.flatten())\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = X_test_b @ beta\n",
    "    \n",
    "    # R2 Score\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    print(\"R2 Score:\", score)\n",
    "    \n",
    "    r2_scores.append(score)\n",
    "    betas.append(beta)\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69414d05",
   "metadata": {},
   "source": [
    "(e) Use the best value of (ùõΩ) matrix (for which R2_score is maximum), to train the regressor for 70% of data and test the performance for remaining 30% data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade5f83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(e) Best R2 Score: 0.9243869413350317\n",
      "(e) Best Beta Matrix:\n",
      " [1.23161736e+06 2.30225051e+05 1.63956839e+05 1.21115120e+05\n",
      " 7.83467170e+02 1.50662447e+05]\n"
     ]
    }
   ],
   "source": [
    "best_idx = np.argmax(r2_scores)\n",
    "best_beta = betas[best_idx]\n",
    "\n",
    "print(\"\\n(e) Best R2 Score:\", r2_scores[best_idx])\n",
    "print(\"(e) Best Beta Matrix:\\n\", best_beta.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a7b04",
   "metadata": {},
   "source": [
    "Part 2 : Concept of Validation set for Multiple Linear Regression (Gradient Descent Optimization) Consider the same dataset of Q1, rather than dividing the dataset into five folds, divide the dataset into training set (56%), validation set (14%), and test set (30%). Consider four different values of learning rate i.e. {0.001,0.01,0.1,1}. Compute the values of regression coefficients for each value of learning rate after 1000 iterations. For each set of regression coefficients, compute R2_score for validation and test set and find the best value of regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0a4f9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate = 0.001\n",
      "Beta coefficients (rounded): [1065976.39  201488.21  140561.4    97763.8    20752.23  130367.98]\n",
      "R2 Score (Validation): 0.6874\n",
      "R2 Score (Test): 0.6523\n",
      "\n",
      "Learning Rate = 0.01\n",
      "Beta coefficients (rounded): [1232434.57  234562.96  162415.89  121759.16    2815.32  151577.64]\n",
      "R2 Score (Validation): 0.9098\n",
      "R2 Score (Test): 0.9148\n",
      "\n",
      "Learning Rate = 0.1\n",
      "Beta coefficients (rounded): [1232434.58  234562.99  162415.95  121760.31    2814.16  151577.59]\n",
      "R2 Score (Validation): 0.9098\n",
      "R2 Score (Test): 0.9148\n",
      "\n",
      "Learning Rate = 1\n",
      "Stopped early: Divergence at iteration 18\n",
      "‚ùå Diverged (too large learning rate)\n",
      "\n",
      "‚úÖ Best Learning Rate: 0.01\n",
      "Best Validation R2: 0.9098\n",
      "Best Beta (rounded): [1232434.57  234562.96  162415.89  121759.16    2815.32  151577.64]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv(\"USA_Housing.csv\")\n",
    "\n",
    "X = df.drop(\"Price\", axis=1).values\n",
    "y = df[\"Price\"].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Split Train (56%), Val (14%), Test (30%)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Bias term\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_val_b = np.c_[np.ones((X_val.shape[0], 1)), X_val]\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "\n",
    "# Gradient Descent Function with safe stop\n",
    "\n",
    "def gradient_descent(X, y, lr, n_iter=1000, tol=1e10):\n",
    "    m, n = X.shape\n",
    "    beta = np.zeros((n, 1))\n",
    "    for i in range(n_iter):\n",
    "        gradients = (2/m) * X.T @ (X @ beta - y)\n",
    "        beta -= lr * gradients\n",
    "        # stop if values explode\n",
    "        if np.any(np.abs(beta) > tol):\n",
    "            print(f\"Stopped early: Divergence at iteration {i}\")\n",
    "            return None\n",
    "    return beta\n",
    "\n",
    "\n",
    "# Try learning rates\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 1]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nLearning Rate = {lr}\")\n",
    "    beta = gradient_descent(X_train_b, y_train, lr, n_iter=1000)\n",
    "    \n",
    "    if beta is None:\n",
    "        print(\"‚ùå Diverged (too large learning rate)\")\n",
    "        continue\n",
    "    \n",
    "    # Predictions\n",
    "    y_val_pred = X_val_b @ beta\n",
    "    y_test_pred = X_test_b @ beta\n",
    "    \n",
    "    # Scores\n",
    "    r2_val = r2_score(y_val, y_val_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"Beta coefficients (rounded):\", np.round(beta.flatten(), 2))\n",
    "    print(\"R2 Score (Validation):\", round(r2_val, 4))\n",
    "    print(\"R2 Score (Test):\", round(r2_test, 4))\n",
    "    \n",
    "    results.append((lr, beta, r2_val, r2_test))\n",
    "\n",
    "\n",
    "# Best Result\n",
    "\n",
    "if results:\n",
    "    best_result = max(results, key=lambda x: x[2])\n",
    "    print(\"\\n‚úÖ Best Learning Rate:\", best_result[0])\n",
    "    print(\"Best Validation R2:\", round(best_result[2], 4))\n",
    "    print(\"Best Beta (rounded):\", np.round(best_result[1].flatten(), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4996f",
   "metadata": {},
   "source": [
    "Part 3 : Pre-processing and Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Dataset loaded. Shape: (205, 26)\n",
      "First 5 rows:\n",
      "    symboling  normalized_losses         make fuel_type aspiration num_doors  \\\n",
      "0          3                NaN  alfa-romero       gas        std       two   \n",
      "1          3                NaN  alfa-romero       gas        std       two   \n",
      "2          1                NaN  alfa-romero       gas        std       two   \n",
      "3          2              164.0         audi       gas        std      four   \n",
      "4          2              164.0         audi       gas        std      four   \n",
      "\n",
      "    body_style drive_wheels engine_location  wheel_base  ...  engine_size  \\\n",
      "0  convertible          rwd           front        88.6  ...          130   \n",
      "1  convertible          rwd           front        88.6  ...          130   \n",
      "2    hatchback          rwd           front        94.5  ...          152   \n",
      "3        sedan          fwd           front        99.8  ...          109   \n",
      "4        sedan          4wd           front        99.4  ...          136   \n",
      "\n",
      "   fuel_system  bore  stroke compression_ratio horsepower  peak_rpm city_mpg  \\\n",
      "0         mpfi  3.47    2.68               9.0      111.0    5000.0       21   \n",
      "1         mpfi  3.47    2.68               9.0      111.0    5000.0       21   \n",
      "2         mpfi  2.68    3.47               9.0      154.0    5000.0       19   \n",
      "3         mpfi  3.19    3.40              10.0      102.0    5500.0       24   \n",
      "4         mpfi  3.19    3.40               8.0      115.0    5500.0       18   \n",
      "\n",
      "   highway_mpg    price  \n",
      "0           27  13495.0  \n",
      "1           27  16500.0  \n",
      "2           26  16500.0  \n",
      "3           30  13950.0  \n",
      "4           22  17450.0  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "(2) Missing values handled.\n",
      "Remaining NaN count:\n",
      " 0\n",
      "\n",
      "(3) Non-numeric columns converted.\n",
      "Dataset shape after encoding: (201, 30)\n",
      "\n",
      "(4) Features scaled.\n",
      "X shape: (201, 29) | y shape: (201, 1)\n",
      "\n",
      "(5) Linear Regression without PCA\n",
      "R2 Score on Test Set: 0.8733\n",
      "\n",
      "(6) Linear Regression with PCA\n",
      "Number of components after PCA: 16\n",
      "R2 Score on Test Set: 0.8612\n",
      "\n",
      "‚ùå PCA did not improve the performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitya\\AppData\\Local\\Temp\\ipykernel_26780\\2006570765.py:50: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"num_doors\"] = df[\"num_doors\"].replace(word_to_num)\n",
      "C:\\Users\\nitya\\AppData\\Local\\Temp\\ipykernel_26780\\2006570765.py:51: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"num_cylinders\"] = df[\"num_cylinders\"].replace(word_to_num)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# 1. Load dataset with column names & replace '?' with NaN\n",
    "\n",
    "columns = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\n",
    "           \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\",\n",
    "           \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
    "           \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
    "           \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\",\n",
    "           \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "\n",
    "df = pd.read_csv(\"CarDetail.csv\", names=columns, na_values=\"?\")\n",
    "\n",
    "print(\"(1) Dataset loaded. Shape:\", df.shape)\n",
    "print(\"First 5 rows:\\n\", df.head())\n",
    "\n",
    "\n",
    "# 2. Handle NaN (central tendency imputation)\n",
    "\n",
    "# Drop rows with NaN in price (target variable)\n",
    "df = df.dropna(subset=[\"price\"])\n",
    "\n",
    "# For numeric columns: fill NaN with mean\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# For categorical columns: fill NaN with mode\n",
    "categorical_cols = df.select_dtypes(include=[object]).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "print(\"\\n(2) Missing values handled.\")\n",
    "print(\"Remaining NaN count:\\n\", df.isna().sum().sum())\n",
    "\n",
    "\n",
    "# 3. Convert non-numeric columns\n",
    "\n",
    "\n",
    "# (i) num_doors & num_cylinders: convert word to number\n",
    "word_to_num = {\n",
    "    \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
    "    \"six\": 6, \"eight\": 8, \"twelve\": 12\n",
    "}\n",
    "df[\"num_doors\"] = df[\"num_doors\"].replace(word_to_num)\n",
    "df[\"num_cylinders\"] = df[\"num_cylinders\"].replace(word_to_num)\n",
    "\n",
    "# (ii) body_style, drive_wheels ‚Üí dummy encoding\n",
    "df = pd.get_dummies(df, columns=[\"body_style\", \"drive_wheels\"], drop_first=True)\n",
    "\n",
    "# (iii) make, aspiration, engine_location, fuel_type ‚Üí label encoding\n",
    "label_cols = [\"make\", \"aspiration\", \"engine_location\", \"fuel_type\"]\n",
    "for col in label_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# (iv) fuel_system: \"pfi\" ‚Üí 1, else 0\n",
    "df[\"fuel_system\"] = df[\"fuel_system\"].apply(lambda x: 1 if \"pfi\" in x else 0)\n",
    "\n",
    "# (v) engine_type: \"ohc\" ‚Üí 1, else 0\n",
    "df[\"engine_type\"] = df[\"engine_type\"].apply(lambda x: 1 if \"ohc\" in x else 0)\n",
    "\n",
    "print(\"\\n(3) Non-numeric columns converted.\")\n",
    "print(\"Dataset shape after encoding:\", df.shape)\n",
    "\n",
    "\n",
    "# 4. Divide dataset into input (X) and output (y)\n",
    "\n",
    "X = df.drop(\"price\", axis=1).values\n",
    "y = df[\"price\"].values.reshape(-1, 1)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\n(4) Features scaled.\")\n",
    "print(\"X shape:\", X_scaled.shape, \"| y shape:\", y.shape)\n",
    "\n",
    "\n",
    "# 5. Train/Test Split (70% train, 30% test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.30, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "r2_original = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n(5) Linear Regression without PCA\")\n",
    "print(\"R2 Score on Test Set:\", round(r2_original, 4))\n",
    "\n",
    "\n",
    "# 6. PCA + Linear Regression\n",
    "\n",
    "pca = PCA(n_components=0.95)  # keep 95% variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.30, random_state=42)\n",
    "\n",
    "model_pca = LinearRegression()\n",
    "model_pca.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "y_pred_pca = model_pca.predict(X_test_pca)\n",
    "r2_pca = r2_score(y_test_pca, y_pred_pca)\n",
    "\n",
    "print(\"\\n(6) Linear Regression with PCA\")\n",
    "print(\"Number of components after PCA:\", X_pca.shape[1])\n",
    "print(\"R2 Score on Test Set:\", round(r2_pca, 4))\n",
    "\n",
    "# Compare\n",
    "if r2_pca > r2_original:\n",
    "    print(\"\\n‚úÖ PCA improved the performance.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå PCA did not improve the performance.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
